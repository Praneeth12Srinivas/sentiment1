NHS Twitter Sentiment Analysis
==============================

```{r init,echo=FALSE,output=FALSE}
opts_chunk$set(cache=TRUE)
options(warn=-1)
```


Preliminaries
---------------

You'll need the code I've put in a package called `sentiment` on github. This is easiest to install using the `devtools` package.

```{r reqs,eval=FALSE}
install.packages("devtools")
require(devtools)
install_github("sentiment1","spacedman")
```


You also need a few prerequisites - install these if you haven't got them:


```{r forstarters}
require(twitteR)
require(RColorBrewer)
require(plyr)
```

Read The Latest Tweets
------------------------

We'll just use the `twitteR` package to get the most recent tweets with the `nhs` hashtag. 

```{r get_tweets}
nhsTweets=searchTwitter("#nhs",n=1500)
```

Compute The Score
-------------------

For this I'm using a word list created by the [Computational Story Lab](http://onehappybird.com/2012/03/19/does-qwerty-affect-happiness/). Each word has been rated happy or sad by Amazon's Mechanical Turk process, and the score for each tweet is the average score of all the words in the tweet that appear in the list.

I also do a bit of filtering to take out tweets that don't have any of the words in the list, and I also remove duplicate tweets. I also filter out the words "nhs" and "via" (which appears when a tweet is passed on) as well as retweets (anything starting with "rt" and space, URLs, punctuation, numbers, and control characters.

That gives us a number of tweets with a score and a created date.

```{r score_tweets,output=FALSE,warning=FALSE}
data(labMT)
LMScoref = fScoreLM(labMT,"happiness_average")
setScoreTweets(nhsTweets,LMScoref)
nhs = ldply(nhsTweets,function(x){data.frame(day=as.Date(x$created),created=x$created,score=x$score,text=x$text)})
nhs = nhs[!is.na(nhs$score),]
nhs$filtered=laply(nhs$text,function(x){filter(x)})
nhs = nhs[!duplicated(nhs$filtered),]
nhs = nhs[order(nhs$score),]
```

A Few Samples
--------------

Let's see what we've got. First, the unhappiest tweets:

* `r nhs[1,c("text")]`
* `r nhs[2,c("text")]`
* `r nhs[3,c("text")]`
* `r nhs[4,c("text")]`
* `r nhs[5,c("text")]`

and then the happiest:

* `r nhs[nrow(nhs),c("text")]`
* `r nhs[nrow(nhs)-1,c("text")]`
* `r nhs[nrow(nhs)-2,c("text")]`
* `r nhs[nrow(nhs)-3,c("text")]`
* `r nhs[nrow(nhs)-4,c("text")]`

Simple Plots
--------------

This leaves us with a total of `r nrow(nhs)` tweets. Let's plot them over time and look at a histogram of the score.

```{r firstplots}
ggplot(data=nhs,aes(x=created))+geom_histogram()

ggplot(data=nhs,aes(x=score)) + geom_histogram(aes(y = ..density..)) + geom_density()
```

Wordclouds
-----------

Everybody loves wordclouds right? Okay, maybe not. Let's divide up the tweets into three equal-sized groups ordered by score and do a wordcloud of the two extreme groups.


```{r quants,output=FALSE}
qs = quantile(nhs$score,probs=seq(0,1,len=4),na.rm=TRUE)
nhs$scoreQ = cut(nhs$score,qs)
pal = brewer.pal(11,"Spectral")
p1 = brewer.pal(9,"OrRd")[5:9]
p2 = brewer.pal(9,"PuBu")[5:9]
wordcloud(nhs[as.numeric(nhs$scoreQ)==1,]$filtered,max.words=100,color=p1,random.order=FALSE)
wordcloud(nhs[as.numeric(nhs$scoreQ)==3,]$filtered,max.words=100,color=p2,random.order=FALSE)

```

First we have the words found in the unhappiest tweets, and then the words found in the happiest tweets. 
